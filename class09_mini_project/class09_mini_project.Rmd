---
title: "class09_mini_project"
author: "Zaida Rodriguez (PID:A59010549)"
date: "10/27/2021"
output: github_document
---

load in data
```{r}
read.csv("WisconsinCancer.csv")
fna.data <- "WisconsinCancer.csv"
```

input and store as wisc.df
```{r}
wisc.df <- read.csv(fna.data, row.names=1)
```

double check the row names
```{r}
head(wisc.df)
```
We dont want the diagnosis column in our data analysis but since we want to use this data later on, create a new vector for diagnosis
```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
diagnosis
```

```{r}
table(diagnosis)
```

We do not want the first column in our analysis, therefore, remove it.
Use [,-1] or $
```{r}
wisc.df <- wisc.df[,-1]
```

> Q1. How many observations are in this dataset?
569

> Q2. How many observations have malignant diagnosis?
212

> Q3. How many variables/features in the data are suffixed with 
_mean?
10

check out the columns name to see the names
we want to know the length of the vector
```{r}
colnames(wisc.df) # shows the naems of the different columns
grep("_mean", colnames(wisc.df)) # lists which columns contain _mean
length( grep("_mean", colnames(wisc.df))) # tells us how many columns contain _mean
```

## PCA 

first check to see if the data is scaled
```{r}
colMeans(wisc.df)
```

notice there is a X column, you need to remove it
```{r}
ncol(wisc.df) #to see the number of columns
wisc.df <- wisc.df[,-31] # to remove the number of columns
wisc.df
ncol(wisc.df)
```

okay now scale the data
```{r}
apply(wisc.df,2,sd) # shows the sd of each 
wisc.pr <- prcomp(wisc.df, scale. = TRUE) # scales the data when you tell it to (true)
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427 

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PC

```{r}
x <- summary(wisc.pr)
x$importance[3,]
x$importance[3,] >= 0.7
which(x$importance[3,] >= 0.7)
y <- which(x$importance[3,] >= 0.7)
y[1]
```

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PC 

```{r}
x <- summary(wisc.pr)
x$importance[3,]
x$importance[3,] >= 0.9
which(x$importance[3,] >= 0.9)
y <- which(x$importance[3,] >= 0.9)
y[1]
```

Now create a biplot function
```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

There is too much clutter, it is too congested and it is difficult to get an idea of how the variance/relationship between the data.

There is too much going on with that plot, so lets clean it up a little by doing a scatter plot
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
plot(wisc.pr$x[,c(1,3)], col=diagnosis, xlab = "PC1", ylab = "PC3") # you have to c() to t
```
> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

The plot comparing PC1 to PC3, show there is more variance/difference between 

create a data frame for ggplot
```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```

load ggplot library
```{r}
library(ggplot2)
```

make a scatter plot colored by diagnosis
```{r}
ggplot(df) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

now calculate the variance of each component
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

variance explained by each principal component
```{r}
total.var <- sum(pr.var)
pve <- pr.var/ total.var
pve
```

plot the variance explained for each principal component
```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Alternative scree plot of the same data, note data driven y-axis
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation[,1]
```
The feature concave.points_mean = -0.26085376

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

5 PC 
```{r}
summary(wisc.pr)
```

First scale the data using the "scale()" function
```{r}
data.scaled <- scale(wisc.df)
```


Calculate the distance between observations
```{r}
data.dist <- dist(data.scaled)
```

Now create a hierarchical cluster using complete linkage.
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

height = 20 

```{r}
plot(wisc.hclust)
abline(h= 20, col="red", lty=2)
```
Now cut the tree
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=20)
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=24)
table(wisc.hclust.clusters, diagnosis)
```

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

Complete gives me a better idea of whats going on but ward.D2 is also really good. I am able to better follow the clusters with complete, but ward.D2 gives some separation between the text and and the clusters which makes it easy to follow, 

```{r}
x <-hclust(data.dist, method = "complete")
plot(x)
```
## 5. Clustering on PCA results 

make a new dendrogram
```{r}
wisc.pr.hclust <- hclust(data.dist, method = "ward.D2")
plot(wisc.pr.hclust)
```


Cut in order to haved 2 clusters 
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
g <- as.factor(grps)
levels(g)

#flip
g <- relevel(g,2)
levels(g)
```


```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
#plot with reversed order
plot(wisc.pr$x[,1:2], col=g)
```

Use clustering along the first 7 PCS  
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2" )
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters, diagnosis)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:2]), method="ward.D2")
table(wisc.pr.hclust.clusters, diagnosis)
```

# 6. Sensitivity/Specificity 
**accuary**, essentially how many did we get correct?
```{r}
(165+351)/nrow(wisc.df)
```

Sensitivity TP/(TP+FN)
```{r}
(164)/(164+48)
```


## 7. Prediction
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

Patient 2 should be prioritized

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], labels=c(1,2), col="white")
```

